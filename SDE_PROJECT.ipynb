{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arpit4477/p22io201/blob/main/SDE_PROJECT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tY3M4oahmz8",
        "outputId": "393e5c52-de06-4d4b-93ff-03d53c2cf74c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSRqFMDsfqb5",
        "outputId": "708732a9-b2f3-42ca-e15f-cfeb5db3c6b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Could not open camera.\n",
            "Error: Could not read frame.\n"
          ]
        }
      ],
      "source": [
        "#google_drive_link1 = '/content/drive/MyDrive/ColabDataSet/coco.names'\n",
        "#google_drive_link2 = '/content/drive/MyDrive/ColabDataSet/yolov3.cfg'\n",
        "#google_drive_link3 = '/content/drive/MyDrive/ColabDataSet/yolov3.weights'\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Load YOLO\n",
        "net = cv2.dnn.readNetFromDarknet(\"/content/drive/MyDrive/ColabDataSet/yolov3.cfg\", \"/content/drive/MyDrive/ColabDataSet/yolov3.weights\")\n",
        "classes = []\n",
        "with open(\"/content/drive/MyDrive/ColabDataSet/coco.names\", \"r\") as f:\n",
        "    classes = [line.strip() for line in f.readlines()]\n",
        "layer_names = net.getLayerNames()\n",
        "output_layers = net.getUnconnectedOutLayers()\n",
        "\n",
        "\n",
        "# Capturing video\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "if not cap.isOpened():\n",
        "    print(\"Error: Could not open camera.\")\n",
        "\n",
        "\n",
        "while True:\n",
        "    _, frame = cap.read()\n",
        "    if frame is None:\n",
        "        print(\"Error: Could not read frame.\")\n",
        "        break\n",
        "\n",
        "        height, width, channels = frame.shape\n",
        "\n",
        "\n",
        "    # Extract the right 33% of the frame\n",
        "    right_frame = frame[:, int(2 * width / 3):]\n",
        "\n",
        "\n",
        "    # Detect objects using YOLO on the right side\n",
        "    blob = cv2.dnn.blobFromImage(right_frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
        "    net.setInput(blob)\n",
        "    output_layers_names = net.getUnconnectedOutLayersNames()\n",
        "    outs = net.forward(output_layers_names)\n",
        "\n",
        "\n",
        "    # Get information from the detection\n",
        "    class_ids = []\n",
        "    confidences = []\n",
        "    boxes = []\n",
        "\n",
        "\n",
        "    # Process each output layer\n",
        "    for out in outs:\n",
        "        for detection in out:\n",
        "            scores = detection[5:]\n",
        "            class_id = np.argmax(scores)\n",
        "            confidence = scores[class_id]\n",
        "            if confidence > 0.5:  # Adjust confidence threshold as needed\n",
        "                # Object detected\n",
        "                center_x = int(detection[0] * (width / 3)) + int(2 * width / 3)  # Adjust x-coordinate for the full frame\n",
        "                center_y = int(detection[1] * height)\n",
        "                w = int(detection[2] * (width / 3))\n",
        "                h = int(detection[3] * height)\n",
        "\n",
        "\n",
        "                # Get the coordinates of the object\n",
        "                x = int(center_x - w / 2)\n",
        "                y = int(center_y - h / 2)\n",
        "\n",
        "\n",
        "                # Store information for displaying bounding box and text\n",
        "                boxes.append([x, y, w, h])\n",
        "                confidences.append(float(confidence))\n",
        "                class_ids.append(class_id)\n",
        "\n",
        "\n",
        "    # Apply non-maximum suppression to remove overlapping bounding boxes\n",
        "    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
        "\n",
        "\n",
        "    # Display names and bounding boxes of the detected objects\n",
        "    for i in range(len(boxes)):\n",
        "        if i in indices:\n",
        "            x, y, w, h = boxes[i]\n",
        "\n",
        "\n",
        "            # Draw bounding box and name\n",
        "            label = f\"{classes[class_ids[i]]}: {confidences[i]:.2f}\"\n",
        "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "            cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "\n",
        "            # Apply perspective transformation based on the detected object's corners (optional)\n",
        "            imgPts = np.float32([[x, y], [x + w, y], [x, y + h], [x + w, y + h]])\n",
        "            objPoints = np.float32([[0, 0], [420, 0], [0, 637], [420, 637]])\n",
        "            matrix = cv2.getPerspectiveTransform(imgPts, objPoints)\n",
        "            result = cv2.warpPerspective(frame, matrix, (400, 600))\n",
        "\n",
        "\n",
        "            # Display the original and corrected frames\n",
        "            cv2.imshow('Object Detection', frame)\n",
        "            cv2.imshow('Perspective Transformation', result)\n",
        "\n",
        "\n",
        "    # Continue capturing frames even if no objects are detected\n",
        "\n",
        "\n",
        "    # Check for user exit (press 'Esc' key)\n",
        "    key = cv2.waitKey(1)\n",
        "    if key == 27:\n",
        "        break\n",
        "\n",
        "\n",
        "# Release video capture and close windows\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    }
  ]
}